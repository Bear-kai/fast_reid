MODEL:
  META_ARCHITECTURE: "Baseline"
  WEIGHTS: ""   # 用于恢复训练，注意：这里为空backbone也可以采用预训练模型

  BACKBONE:
    # NAME: "build_resnet_backbone"  
    # NORM: "BN"
    # DEPTH: "50x"
    # LAST_STRIDE: 1
    # FEAT_DIM: 2048 
    # WITH_IBN: False
    # PRETRAIN: True
    # PRETRAIN_PATH: "/data_4t/xk/classification/pretrained_models/resnet50-19c8e357.pth"

    # 选用mobilefacenet(_air)时, 须设置HEADS.POOL_LAYER='identity'
    # NAME: 'build_mobilefacenet_air' # 'build_mobilefacenet_air'  #  
    # POOL_TYPE: 'GDConv'          # 'GDConv'  'GAP'  'GMP'  'GAMP_add'  'GAMP_cat'    *****
    # SETTING_STR: 'MobileFaceNet' # 该参数对_air无效
    # L7SCALE: 1                   # for GAMP_cat, 输出特征维度是否加倍                  *****
    # FEAT_DIM: 128                # for GAMP_cat: 128*L7SCALE ，for其他：128           *****
    # CBAM: False
    # PRETRAIN: False
    # PRETRAIN_PATH: ''

    # 选用shufflenetv2时, 须设置HEADS.POOL_LAYER='avgpool'/'maxpool'/'avgmaxpool'
    NAME: 'build_shufflenetv2'    # 'build_mobilefacenet_air'  #  
    MODEL_SIZE: '1.0x'            # '0.5x'  '1.0x'  '1.5x'  '2.0x'
    POOL_LAYER: 'pool_s2'         # 'pool_s2'  'pool_s1'  'no_pool'                  *****
    FEAT_DIM: 1024                # for GAMP_cat: 128*L7SCALE ，for其他：128           *****
    PRETRAIN: False
    PRETRAIN_PATH: ''

  HEADS:
    NAME: "EmbeddingHead"
    NORM: "BN"
    WITH_BNNECK: True
    POOL_LAYER: "avgpool"   # avgpool  maxpool  avgmaxpool  identity      *****
    NECK_FEAT: "before"      # before  after 
    CLS_LAYER: "linear"

  LOSSES:
    NAME: ("CrossEntropyLoss", "TripletLoss",)

    CE:
      EPSILON: 0.1
      SCALE: 1.

    TRI:
      MARGIN: 0.3
      HARD_MINING: True
      NORM_FEAT: False
      SCALE: 1.

INPUT:
  SIZE_TRAIN: [256, 128]
  SIZE_TEST: [256, 128]
  REA:
    ENABLED: True
    PROB: 0.5
    MEAN: [123.675, 116.28, 103.53]
  DO_PAD: True

DATALOADER:
  PK_SAMPLER: True              # True: triplet_sampler
  NAIVE_WAY: True               # True：NaiveIdentitySampler    False：BalancedIdentitySampler
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPT: "SGD"                   # "Adam"  "SGD" 
  MOMENTUM: 0.9                 # for SGD
  MAX_ITER: 120
  BASE_LR: 0.01               # 0.00035  0.01
  BIAS_LR_FACTOR: 2.            # 2.   why double lr for bias parameters ???
  HEADS_LR_FACTOR: 1.           # 
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0005     
  IMS_PER_BATCH: 64             # batch_size 64 *****

  SCHED: "WarmupMultiStepLR"
  STEPS: [40, 70]               # [40,90]   论文中的是[40,70]
  GAMMA: 0.1

  WARMUP_FACTOR: 0.01           # warm_lr = 0.01*base_lr --> 1.0*base_lr
  WARMUP_ITERS: 10

  CHECKPOINT_PERIOD: 60

TEST:
  EVAL_PERIOD: 30
  IMS_PER_BATCH: 128

DATASETS:
  NAMES: ("Market1501",)
  TESTS: ("Market1501",)

# OUTPUT_DIR: "logs/market1501/bagtricks_R50_20201206_Adam_lr0p00035_factor1_step70_1gpu"
# OUTPUT_DIR: "logs/market1501/23_bagtricks_mobileFNair_GDConv_SGD_lr0p01_kd"
OUTPUT_DIR: "logs/market1501/25_bagtricks_shufflev2_1p0x_GAP_SGD_lr0p01_kd"

CUDNN_BENCHMARK: True

